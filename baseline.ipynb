{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "narrative-emphasis",
   "metadata": {
    "id": "narrative-emphasis"
   },
   "source": [
    "<p>It is advisable that you read our introductory documentation webpage before moving on with understading the code. As it would help you understand the problem better.</p>\n",
    "<p>You can check it out <a href=\"https://hasocfire.github.io/hasoc/2021/ichcl/index.html\">here</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-violation",
   "metadata": {
    "id": "continuing-violation"
   },
   "source": [
    "### Importing Libraries and initializing stopwords and stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fossil-enlargement",
   "metadata": {
    "id": "fossil-enlargement"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk.stem  as hindi_stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-kitchen",
   "metadata": {
    "id": "greenhouse-kitchen"
   },
   "source": [
    "<p>Making a list of english and hindi stopwords. <br>The enlgish stopwords are retrieved from NLTK library as well. <br>And the hindi stopwords are retrieved from a data set on Mendeley Data. To read about how the authors compiled the list, you can check their <a href = \"https://arxiv.org/ftp/arxiv/papers/2002/2002.00171.pdf\" > publicaion </a> </p>\n",
    "<p>Initializing an english SnowballStemmer using the NLTK library. <br>And the hindi stemmer used was produced by students of Banasthali University. You can check out their <a href=\"https://arxiv.org/ftp/arxiv/papers/1305/1305.6211.pdf\">publication</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "gentle-richards",
   "metadata": {
    "id": "gentle-richards"
   },
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words(\"english\")\n",
    "with open('final_stopwords.txt', encoding = 'utf-8') as f:\n",
    "    hindi_stopwords = f.readlines()\n",
    "    for i in range(len(hindi_stopwords)):\n",
    "        hindi_stopwords[i] = re.sub('\\n','',hindi_stopwords[i])\n",
    "stopwords = english_stopwords + hindi_stopwords\n",
    "english_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YeWusmHa9aFc",
   "metadata": {
    "id": "YeWusmHa9aFc"
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-bristol",
   "metadata": {
    "id": "appreciated-bristol"
   },
   "source": [
    "<p>Initializing a list of various directories that data is stored in using the glob Library.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-stocks",
   "metadata": {
    "id": "voluntary-stocks"
   },
   "source": [
    "<p>Reading tree structured data from the directories from the .json files</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-algorithm",
   "metadata": {
    "id": "threatened-algorithm"
   },
   "source": [
    "</p>Defining 2 functions that will turn the data from a tree structure to a flat structure.</p>\n",
    "<ul>\n",
    "    <li>tr_flatten: This is to flat the train data. It takes two variables as function parameters. First one is the tweet data and second one is labels. It'll create a list of json structures like following:\n",
    "        <ul>\n",
    "            <li> for source tweet: It'll create json with tweet_id, tweet text and label. </li>\n",
    "            <li> for comment: It'll create json with tweet_id, label and for the text part it'll append the comment after the source tweet. This is a basic technique to provide context of source tweet. </li>\n",
    "            <li> for reply: It'll create json with tweet_id, label and for the text part it'll append the reply after the comment after the source tweet. So the text here will look like \"source tweet-comment-reply\"</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>te_flatten: This is to flat the test data. It works similarly like tr_flatten but without the labels file, as labels won't be available for test set. It'll be used once the test data is available</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-catholic",
   "metadata": {
    "id": "cardiac-catholic"
   },
   "source": [
    "<p>This cell will run both the flatten functions. Again, you can skip the test part if it is not available. The train_len variable will be used later on for splitting the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "limiting-lighter",
   "metadata": {
    "id": "limiting-lighter"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('hi_Hasoc2021_train.csv')\n",
    "df2= pd.read_excel('hasoc2019_hi_test_gold_2919.xlsx')\n",
    "df3=pd.read_excel('hasoc_2020_hi_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b1a2086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3['task_1'] = df3['task_1'].apply((lambda x: re.sub('HOF','1',x)))\n",
    "df3['task_1'] = df3['task_1'].apply((lambda x: re.sub('NOT','0',x)))\n",
    "df3['task_1'] =pd.to_numeric(df3['task_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fe20e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict1 = { 'Hsbinary':df3['task_1'],'Comment':df3['text']}\n",
    "df = pd.DataFrame(my_dict1)\n",
    "df\n",
    "df.to_csv('hi_2010hasoc_binary.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "functioning-asthma",
   "metadata": {
    "id": "functioning-asthma",
    "outputId": "5f577c5d-2df1-47ff-c300-6f65b7aa4c45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4594"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "97982991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text task_1\n",
       "0  वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...    NOT\n",
       "1  #कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...    HOF\n",
       "2  पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...    HOF\n",
       "3  जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...    NOT\n",
       "4  नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...    NOT"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0a2e3a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 आदमीं को मारने पर गोडसे आतंकी हो सके है तो\\n...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Vishesh4: @jawaharyadavbjp जवाहर यादव, अगर...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @FunKeyBaat: #भगवा वस्त्र पहन कर मतदान नही ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yey nina khothani labafazi benu phambili Finis...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Rajeshbhanjan2: जब भी कोई सिकुलर कोंग्रेसी...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text task_1\n",
       "0  1 आदमीं को मारने पर गोडसे आतंकी हो सके है तो\\n...    HOF\n",
       "1  RT @Vishesh4: @jawaharyadavbjp जवाहर यादव, अगर...    NOT\n",
       "2  RT @FunKeyBaat: #भगवा वस्त्र पहन कर मतदान नही ...    HOF\n",
       "3  Yey nina khothani labafazi benu phambili Finis...    HOF\n",
       "4  RT @Rajeshbhanjan2: जब भी कोई सिकुलर कोंग्रेसी...    HOF"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d7a5115d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-b54e6301ad35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd3' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df=pd.concat([df1,d3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b9c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-batman",
   "metadata": {
    "id": "searching-batman",
    "outputId": "82708ad2-d18d-4f3b-9a01-33d8beb75b16"
   },
   "outputs": [],
   "source": [
    "# df['tweet_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-machinery",
   "metadata": {
    "id": "treated-machinery"
   },
   "outputs": [],
   "source": [
    "tweets = df.text\n",
    "y = df.task_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_english_hindi_emojis=\"[^a-zA-Z#\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF\\u0900-\\u097F]\"\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\",' ', tweet)\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\",' ', tweet)\n",
    "    tweet = re.sub(regex_for_english_hindi_emojis,' ', tweet)\n",
    "    tweet = re.sub(\"RT \", \" \", tweet)\n",
    "    tweet = re.sub(\"\\n\", \" \", tweet)\n",
    "    tweet = re.sub(r\" +\", \" \", tweet)\n",
    "    tokens = []\n",
    "    for token in tweet.split():\n",
    "        if token not in stopwords:\n",
    "            token = english_stemmer.stem(token)\n",
    "            #token = hindi_stemmer.hi_ste(token)\n",
    "            tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "COEGvpuj9hZ5",
   "metadata": {
    "id": "COEGvpuj9hZ5"
   },
   "source": [
    "## Preprocessing and featuring the raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-dress",
   "metadata": {
    "id": "conscious-dress"
   },
   "source": [
    "<p>This is a preprocessing function and the regex will match with anything that is not English, Hindi and Emoji.</p>\n",
    "<p>The preprocessing steps are as followed:</p>\n",
    "<ul>\n",
    "    <li>Remove Handles</li>\n",
    "    <li>Remove URLs</li>    \n",
    "    <li>Remove anything that is not English, Hindi and Emoji</li>    \n",
    "    <li>Remove RT which appears in retweets</li>    \n",
    "    <li>Remove Abundant Newlines</li>    \n",
    "    <li>Remove Abundant whitespaces</li>    \n",
    "    <li>Remove Stopwords</li>\n",
    "    <li>Stem English text</li>\n",
    "    <li>Stem Hindi text</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-gateway",
   "metadata": {
    "id": "august-gateway"
   },
   "source": [
    "<p>Using TF-IDF for featuring the text. The vectorizer will only consider vocab terms that appear in more than 5 documents.</p>\n",
    "<p>To learn more about TF-IDF you can check <a href = \"https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558\">here</a> and <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-hudson",
   "metadata": {
    "id": "floating-hudson"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5)\n",
    "X = vectorizer.fit_transform(cleaned_tweets)\n",
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wJ8kmLB29mbC",
   "metadata": {
    "id": "wJ8kmLB29mbC"
   },
   "source": [
    "## Training and evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-intake",
   "metadata": {
    "id": "periodic-intake"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.00001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-airfare",
   "metadata": {
    "id": "compatible-airfare"
   },
   "source": [
    "<p>Training the Logistic Regression classifier provided by Scikit-Learn library.</p>\n",
    "<p>To learn more about Logistic Regression classifier you can check <a href = \"https://www.youtube.com/watch?v=yIYKR4sgzI8\">here</a> and <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-subsection",
   "metadata": {
    "id": "emotional-subsection",
    "outputId": "63e90d48-dfe5-411c-8d66-b9a9ea267589"
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-soccer",
   "metadata": {
    "id": "connected-soccer"
   },
   "source": [
    "<p>Predicting and priting classification metrics for validation set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-appreciation",
   "metadata": {
    "id": "arabic-appreciation"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-intersection",
   "metadata": {
    "id": "challenging-intersection",
    "outputId": "2e8bea3f-60e2-4863-909f-1591f9b2e04a"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() #label encoding labels for training Dense Neural Network\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.compile('adam', loss='binary_crossentropy', metrics = ['accuracy']) #compiling a neural network with 3 layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred = (y_pred > 0.5).astype('int64')\n",
    "y_pred = y_pred.reshape(len(y_pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-affect",
   "metadata": {
    "id": "satisfactory-affect"
   },
   "source": [
    "## Predicting test data and making a sample submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-smile",
   "metadata": {},
   "source": [
    "<p>This part will be used to read and make predictions on the test data once the it is made available. When it is available, make a directory in data directory as 'test' and copy the story direcotries into the test directory.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-arbitration",
   "metadata": {},
   "source": [
    "<p>The test directories do not contain labels.json file so labels list is not initialized for test data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-employment",
   "metadata": {},
   "source": [
    "<p>Flattening the test data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('hi_Hasoc2021_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = test_df.text\n",
    "tweet_ids = test_df.tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = [clean_tweet(tweet) for tweet in test_tweets]\n",
    "cleaned_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(cleaned_test)\n",
    "X_test = X_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-fleet",
   "metadata": {
    "id": "crude-fleet"
   },
   "outputs": [],
   "source": [
    "submission_prediction = classifier.predict(X_test)\n",
    "print(submission_prediction)\n",
    "p=submission_prediction\n",
    "\n",
    "print(p)\n",
    "\n",
    "l=[]\n",
    "ids=[]\n",
    "x=0\n",
    "for i in range(len(p)):\n",
    "    if p[i]=='NOT':\n",
    "        l.append('NOT')\n",
    "    if p[i]=='HOF':\n",
    "        l.append('HOF')\n",
    "        x=x+1\n",
    "        \n",
    "    ids.append(test_df['tweet_id'][i])\n",
    "    \n",
    "print(x)\n",
    "\n",
    "my_dict1 = { 'id':ids,'label':l}\n",
    "df = pd.DataFrame(my_dict1)\n",
    "df\n",
    "# df.to_csv('hi_neural_network_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c880a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "continuing-violation",
    "YeWusmHa9aFc",
    "COEGvpuj9hZ5",
    "wJ8kmLB29mbC",
    "satisfactory-affect"
   ],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
